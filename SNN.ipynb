{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5cAtvnSETB2v/flo8BwWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HelenaBahrami/GoatVentures/blob/master/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spiking Neural Network Components and Their Implementation Using PyTorch\n",
        "Spiking Neural Networks (SNNs) are a biologically inspired model of neural networks where information is transmitted via discrete spikes rather than continuous values, mimicking how neurons in the brain communicate. This makes them suitable for neuromorphic computing and energy-efficient AI systems. The key components of SNNs include encoding methods, neuron models, and learning algorithms, which can all be implemented using PyTorch. Here's an overview of these components and their implementation:\n",
        "\n",
        "**Encoding:** BSA (Bens Spiking Algorithm)\n",
        "The encoding mechanism in SNNs converts continuous input data into spike trains. The BSA (Bens Spiking Algorithm) is an effective method to achieve this. It maps real-valued inputs into a series of spikes based on their intensity and timing, which are then used by the spiking neurons for further processing.\n",
        "\n",
        "**Neuron Model:** Leaky Integrate-and-Fire (LIF)\n",
        "The Leaky Integrate-and-Fire (LIF) neuron model is one of the most widely used in SNNs. It integrates incoming spikes over time, and once the membrane potential crosses a certain threshold, it generates a spike. The neuron \"leaks\" potential over time, which allows it to reset and respond to further inputs. Implementing an LIF model in PyTorch involves simulating this integration, leak, and spike generation behavior using PyTorch tensors and operations.\n",
        "\n",
        "**Learning Algorithm:** Spike-Timing-Dependent Plasticity (STDP)\n",
        "Spike-Timing-Dependent Plasticity (STDP) is a biologically inspired learning rule used in SNNs. It adjusts the synaptic weights based on the relative timing of spikes from the pre- and post-synaptic neurons. If a pre-synaptic neuron fires shortly before a post-synaptic neuron, the synapse is strengthened (long-term potentiation), whereas if the pre-synaptic neuron fires after the post-synaptic neuron, the synapse is weakened (long-term depression). This timing-based learning is key for pattern recognition tasks in SNNs and can be implemented in PyTorch by updating synaptic weights using differential timing between neurons.\n",
        "\n",
        "## Overview of Implementation Steps:\n",
        "**Encoding using BSA:**\n",
        "\n",
        "Create a function to encode continuous inputs into spikes based on intensity and timing using PyTorch.\n",
        "\n",
        "**Neuron Model (LIF):**\n",
        "\n",
        "Define the LIF model in PyTorch, including the membrane potential, threshold, and leakage over time.\n",
        "Simulate spike generation and potential reset upon reaching the threshold.\n",
        "\n",
        "**Learning with STDP:**\n",
        "\n",
        "Implement the STDP rule to adjust synaptic weights by calculating the time difference between pre- and post-synaptic spikes.\n",
        "Update the weights using PyTorch’s autograd feature for efficient backpropagation during training.\n",
        "This framework will serve as the foundation for designing SNNs in PyTorch, enabling you to simulate biologically inspired networks for energy-efficient AI tasks."
      ],
      "metadata": {
        "id": "c3I1JfEErM_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Encoder Algorithm\n",
        "The Ben's Spike Algorithm (BSA) is an encoding method for converting analog input signals into binary spike trains. This is particularly useful in Spiking Neural Networks (SNNs), which operate using discrete spikes instead of continuous signals. BSA aims to create a spike train representation of a signal that captures the signal's temporal and magnitude characteristics.\n",
        "\n",
        "The core idea behind BSA is to match a predefined filter to segments of the input signal. When the error between the filter and the signal is below a threshold, a spike is generated. The algorithm iteratively subtracts the filter from the signal after a spike is produced.\n",
        "\n"
      ],
      "metadata": {
        "id": "X1Zfaz9th9z0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed7bKpdF0aaq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the BSA encoding function\n",
        "def BSA_encoding(input_data, filter_type=1, threshold=0.1):\n",
        "    spike_trains = []\n",
        "\n",
        "    # Define filter based on the filter_type\n",
        "    if filter_type == 1:\n",
        "        Filter = torch.tensor([0.8, 0.2])\n",
        "    elif filter_type == 2:\n",
        "        Filter = torch.tensor([0.05] * 6)\n",
        "    elif filter_type == 3:\n",
        "        Filter = torch.tensor([0.8] * 6)\n",
        "\n",
        "    filter_length = len(Filter)\n",
        "\n",
        "    num_samples = input_data.shape[1]\n",
        "\n",
        "    # Loop through all samples\n",
        "    for s in range(num_samples):\n",
        "        sample = input_data[:, s].unsqueeze(1)\n",
        "        timelength = sample.shape[0]\n",
        "        inputnum = sample.shape[1]\n",
        "        time_len = sample.shape[0]\n",
        "        total_error = torch.zeros(inputnum)\n",
        "        SpikeTrain_temp = torch.zeros(timelength, inputnum)\n",
        "\n",
        "        # Signal normalization [0, 1]\n",
        "        min_data = sample.min(0, keepdim=True).values\n",
        "        max_data = sample.max(0, keepdim=True).values\n",
        "        EncodingSignal = (sample - min_data) / (max_data - min_data)\n",
        "\n",
        "        # Set threshold\n",
        "        BSA_Threshold_rowVector = torch.ones(inputnum) * threshold\n",
        "\n",
        "        # Perform BSA encoding\n",
        "        for f in range(inputnum):\n",
        "            for i in range(timelength - filter_length + 1):\n",
        "                error1 = 0\n",
        "                error2 = 0\n",
        "                for j in range(filter_length):\n",
        "                    error1 += abs(EncodingSignal[i + j, f] - Filter[j])\n",
        "                    error2 += abs(EncodingSignal[i + j, f])\n",
        "\n",
        "                if error1 <= (error2 - BSA_Threshold_rowVector[f]):  # Spike criterion\n",
        "                    SpikeTrain_temp[i, f] = 1\n",
        "\n",
        "                    for j in range(filter_length):\n",
        "                        EncodingSignal[i + j, f] -= Filter[j]\n",
        "\n",
        "                    total_error[f] += error1\n",
        "                else:\n",
        "                    total_error[f] += error2\n",
        "\n",
        "        spike_trains.append(SpikeTrain_temp)\n",
        "\n",
        "    return spike_trains\n",
        "\n",
        "# Create a Poisson 2D tensor as input data\n",
        "def generate_poisson_tensor(shape, rate):\n",
        "    return torch.poisson(torch.full(shape, rate, dtype=torch.float))\n",
        "\n",
        "# Plot the Poisson signal and the encoded spike train\n",
        "def plot_poisson_and_encoded(poisson_tensor, spike_trains):\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plot original Poisson signal\n",
        "    axes[0].plot(poisson_tensor[:, 0].numpy(), color='blue')\n",
        "    axes[0].set_title('Original Poisson Signal')\n",
        "\n",
        "    # Plot spike trains\n",
        "    axes[1].stem(spike_trains[0][:, 0].numpy(), 'r', markerfmt='ro', basefmt=\" \")\n",
        "    axes[1].set_title('Spike Train (BSA Encoding)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate Poisson input data\n",
        "input_data = generate_poisson_tensor((100, 1), rate=10)\n",
        "\n",
        "# Perform BSA encoding\n",
        "spike_trains = BSA_encoding(input_data, filter_type=1, threshold=0.1)\n",
        "\n",
        "# Plot results\n",
        "plot_poisson_and_encoded(input_data, spike_trains)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuron Model\n",
        "The Leaky Integrate-and-Fire (LIF) neuron model is a simple yet widely used model in Spiking Neural Networks (SNNs). It mimics the behavior of biological neurons by integrating incoming input signals (synaptic currents) over time.\n",
        "## LIF Characteristics:\n",
        "\n",
        "**Integration:**\n",
        "The neuron continuously integrates the incoming input (current) into its membrane potential. The potential increases as it receives inputs.\n",
        "\n",
        "**Leak:**\n",
        "Over time, the membrane potential gradually \"leaks\" back toward a resting potential if there are no inputs, simulating the natural decay of potential in biological neurons.\n",
        "\n",
        "**Firing (Spike Generation):**\n",
        "Once the membrane potential reaches a certain threshold, the neuron fires, generating a spike (action potential).\n",
        "After firing, the membrane potential is reset to its resting state, and the process starts again."
      ],
      "metadata": {
        "id": "f_oYA4ZZlOle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Leaky Integrate-and-Fire (LIF) neuron model\n",
        "class LIFNeuron:\n",
        "    def __init__(self, tau_m=20.0, v_reset=0.0, v_threshold=1.0, dt=1.0, r_m=1.0):\n",
        "        self.tau_m = tau_m           # Membrane time constant (ms)\n",
        "        self.v_reset = v_reset       # Reset membrane potential (after spike)\n",
        "        self.v_threshold = v_threshold # Firing threshold\n",
        "        self.dt = dt                 # Time step (ms)\n",
        "        self.r_m = r_m               # Membrane resistance\n",
        "        self.v_membrane = v_reset    # Initial membrane potential\n",
        "\n",
        "    def reset(self):\n",
        "        self.v_membrane = self.v_reset\n",
        "\n",
        "    def simulate(self, I, time_steps):\n",
        "        v_trace = []  # To store the membrane potential over time\n",
        "        spikes = []   # To store the spike events\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            dv = (-(self.v_membrane - self.v_reset) + self.r_m * I[t]) / self.tau_m\n",
        "            self.v_membrane += dv * self.dt\n",
        "\n",
        "            if self.v_membrane >= self.v_threshold:\n",
        "                spikes.append(1)\n",
        "                self.v_membrane = self.v_reset  # Reset the potential after spike\n",
        "            else:\n",
        "                spikes.append(0)\n",
        "\n",
        "            v_trace.append(self.v_membrane)\n",
        "\n",
        "        return torch.tensor(v_trace), torch.tensor(spikes)\n",
        "\n",
        "# Function to simulate LIF neuron for both constant and varying input currents\n",
        "def simulate_lif():\n",
        "    time_steps = 100\n",
        "    dt = 1.0\n",
        "\n",
        "    # Create LIF neuron\n",
        "    neuron = LIFNeuron(dt=dt)\n",
        "\n",
        "    # Constant current input\n",
        "    constant_current = torch.full((time_steps,), 1.2)  # Constant current\n",
        "    v_const, spikes_const = neuron.simulate(constant_current, time_steps)\n",
        "\n",
        "    # Reset neuron\n",
        "    neuron.reset()\n",
        "\n",
        "    # Varying current input (sinusoidal pattern)\n",
        "    t = torch.arange(0, time_steps, dt)\n",
        "    varying_current = 1.5 * torch.sin(0.1 * t) + 1.0  # Varying current\n",
        "    v_var, spikes_var = neuron.simulate(varying_current, time_steps)\n",
        "\n",
        "    # Plot membrane potentials and spike trains\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    # Plot constant current\n",
        "    axes[0, 0].plot(t.numpy(), constant_current.numpy(), color='blue')\n",
        "    axes[0, 0].set_title('Constant Current Input')\n",
        "\n",
        "    axes[0, 1].plot(t.numpy(), v_const.numpy(), color='red')\n",
        "    axes[0, 1].set_title('Membrane Potential (Constant Current)')\n",
        "\n",
        "    # Plot varying current\n",
        "    axes[1, 0].plot(t.numpy(), varying_current.numpy(), color='green')\n",
        "    axes[1, 0].set_title('Varying Current Input')\n",
        "\n",
        "    axes[1, 1].plot(t.numpy(), v_var.numpy(), color='orange')\n",
        "    axes[1, 1].set_title('Membrane Potential (Varying Current)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the simulation and plot results\n",
        "simulate_lif()\n"
      ],
      "metadata": {
        "id": "xUQ0Shvbln1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Mechanism: Spike-Timing-Dependent Plasticity (STDP)\n",
        "\n",
        "STDP is a biological learning rule used in spiking neural networks (SNNs) that adjusts the strength of synapses (connections between neurons) based on the precise timing of spikes.\n",
        "\n",
        "## STDP Principles:\n",
        "**Causal Relationship (LTP - Long-Term Potentiation):**\n",
        "If a presynaptic neuron (sending neuron) fires shortly before a postsynaptic neuron (receiving neuron), the synapse is strengthened. This encourages the connection between neurons, increasing synaptic weight.\n",
        "\n",
        "**Anti-Causal Relationship (LTD - Long-Term Depression):**\n",
        "If the presynaptic neuron fires after the postsynaptic neuron, the synapse is weakened, decreasing synaptic weight.\n",
        "\n",
        "**Time-Dependent Rule:**\n",
        "The amount of change in synaptic strength depends on the time difference between the pre- and postsynaptic spikes. The closer in time the spikes are, the greater the change.\n",
        "STDP allows SNNs to learn temporal patterns and correlations in spiking activity, making it an important mechanism for synaptic plasticity and learning in neuromorphic computing and brain-inspired systems."
      ],
      "metadata": {
        "id": "pKn6Zmk2pD-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for STDP\n",
        "tau_plus = 20.0  # Time constant for LTP (ms)\n",
        "tau_minus = 20.0  # Time constant for LTD (ms)\n",
        "A_plus = 0.005   # Maximum weight change for LTP\n",
        "A_minus = 0.005  # Maximum weight change for LTD\n",
        "w_max = 1.0  # Maximum synaptic weight\n",
        "w_min = 0.0  # Minimum synaptic weight\n",
        "\n",
        "# Initial synaptic weight\n",
        "w = 0.5  # Initial synaptic weight (start at middle value)\n",
        "\n",
        "# STDP Function\n",
        "def stdp(delta_t):\n",
        "    \"\"\"STDP update rule based on timing difference delta_t.\"\"\"\n",
        "    if delta_t > 0:\n",
        "        # Pre-synaptic neuron fires before post-synaptic (LTP)\n",
        "        return A_plus * np.exp(-delta_t / tau_plus)\n",
        "    else:\n",
        "        # Post-synaptic neuron fires before pre-synaptic (LTD)\n",
        "        return -A_minus * np.exp(delta_t / tau_minus)\n",
        "\n",
        "# Time differences to evaluate (-50 ms to 50 ms)\n",
        "delta_t_values = np.arange(-50, 51, 1)\n",
        "\n",
        "# Track synaptic weight changes\n",
        "weight_changes = []\n",
        "\n",
        "# Apply STDP for each delta_t\n",
        "for delta_t in delta_t_values:\n",
        "    delta_w = stdp(delta_t)\n",
        "    w_new = w + delta_w\n",
        "\n",
        "    # Clip the weight within [w_min, w_max]\n",
        "    w_new = np.clip(w_new, w_min, w_max)\n",
        "\n",
        "    weight_changes.append(w_new)\n",
        "\n",
        "# Plot the STDP behavior\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(delta_t_values, weight_changes, color='b', marker='o')\n",
        "plt.axhline(0.5, color='k', linestyle='--', label='Initial Weight')\n",
        "plt.title(\"STDP Weight Change Based on Timing Difference\")\n",
        "plt.xlabel(\"Time Difference (Pre - Post) [ms]\")\n",
        "plt.ylabel(\"Synaptic Weight\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a-SybLgruqft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spiking Neural Network with BSA Encoding and STDP Learning: A Comparative Analysis of Poisson Signal Responses\n",
        "\n",
        "This code defines a Spiking Neural Network (SNN) using Leaky Integrate-and-Fire (LIF) neurons, which process inputs encoded using Ben's Spike Algorithm (BSA). The SNN has an architecture with 2 input neurons, 10 hidden neurons, and 1 output neuron. Input-to-hidden synapses (synaptic_weights_ih) and hidden-to-output synapses (synaptic_weights_ho) are initialized with random weights. The input signals are Poisson-distributed, and two signals (A and B) are encoded with BSA. The SNN is trained using Spike-Timing-Dependent Plasticity (STDP), which updates the synaptic weights based on the timing difference between pre- and post-synaptic spikes. The network propagates input spikes through the layers and updates the weights using the STDP learning rule. After training on both signals, the output spike trains for Signal A and Signal B are visualized, and their spike rates are compared to determine how the network responds differently to each input. The spike rate calculation gives a quantitative measure of the output neuron's response.\n"
      ],
      "metadata": {
        "id": "APHA3CEaut8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# BSA Encoding Function\n",
        "def BSA_encoding(input_data, filter_type=1, threshold=0.1):\n",
        "    spike_trains = []\n",
        "    if filter_type == 1:\n",
        "        Filter = torch.tensor([0.8, 0.2])\n",
        "    elif filter_type == 2:\n",
        "        Filter = torch.tensor([0.05] * 6)\n",
        "    elif filter_type == 3:\n",
        "        Filter = torch.tensor([0.8] * 6)\n",
        "\n",
        "    filter_length = len(Filter)\n",
        "    num_samples = input_data.shape[1]\n",
        "\n",
        "    for s in range(num_samples):\n",
        "        sample = input_data[:, s].unsqueeze(1)\n",
        "        timelength = sample.shape[0]\n",
        "        inputnum = sample.shape[1]\n",
        "        total_error = torch.zeros(inputnum)\n",
        "        SpikeTrain_temp = torch.zeros(timelength, inputnum)\n",
        "        min_data = sample.min(0, keepdim=True).values\n",
        "        max_data = sample.max(0, keepdim=True).values\n",
        "        EncodingSignal = (sample - min_data) / (max_data - min_data)\n",
        "        BSA_Threshold_rowVector = torch.ones(inputnum) * threshold\n",
        "\n",
        "        for f in range(inputnum):\n",
        "            for i in range(timelength - filter_length + 1):\n",
        "                error1 = 0\n",
        "                error2 = 0\n",
        "                for j in range(filter_length):\n",
        "                    error1 += abs(EncodingSignal[i + j, f] - Filter[j])\n",
        "                    error2 += abs(EncodingSignal[i + j, f])\n",
        "                if error1 <= (error2 - BSA_Threshold_rowVector[f]):\n",
        "                    SpikeTrain_temp[i, f] = 1\n",
        "                    for j in range(filter_length):\n",
        "                        EncodingSignal[i + j, f] -= Filter[j]\n",
        "        spike_trains.append(SpikeTrain_temp)\n",
        "    return torch.cat(spike_trains, dim=1)\n",
        "\n",
        "# Generate Poisson 2D Tensor\n",
        "def generate_poisson_tensor(shape, rate):\n",
        "    return torch.poisson(torch.full(shape, rate, dtype=torch.float))\n",
        "\n",
        "# Leaky Integrate-and-Fire Neuron\n",
        "class LIFNeuron(nn.Module):\n",
        "    def __init__(self, n_in, tau_m=10.0, v_reset=0.0, v_threshold=0.5, dt=1.0, r_m=2.0):\n",
        "        super(LIFNeuron, self).__init__()\n",
        "        self.tau_m = tau_m\n",
        "        self.v_reset = v_reset\n",
        "        self.v_threshold = v_threshold\n",
        "        self.dt = dt\n",
        "        self.r_m = r_m\n",
        "        self.n_in = n_in\n",
        "\n",
        "    def forward(self, I):\n",
        "        batch_size, n_neurons = I.shape\n",
        "        v_membrane = torch.zeros(batch_size, n_neurons)  # Initialize membrane potential with correct shape\n",
        "\n",
        "        spikes = torch.zeros(batch_size, n_neurons)  # To record spike events\n",
        "        for t in range(batch_size):\n",
        "            dv = (-(v_membrane[t] - self.v_reset) + self.r_m * I[t]) / self.tau_m\n",
        "            v_membrane[t] += dv * self.dt\n",
        "\n",
        "            spikes[t] = (v_membrane[t] >= self.v_threshold).float()\n",
        "            v_membrane[t][spikes[t] > 0] = self.v_reset  # Reset membrane potential after firing\n",
        "\n",
        "        return spikes\n",
        "\n",
        "# SNN Network Definition\n",
        "class SNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SNN, self).__init__()\n",
        "        self.input_neurons = LIFNeuron(10)\n",
        "        self.hidden_neurons = LIFNeuron(10)\n",
        "        self.output_neuron = LIFNeuron(1)\n",
        "\n",
        "        self.synaptic_weights_ih = torch.rand(2, 10) * 2  # Increased weights for more input current\n",
        "        self.synaptic_weights_ho = torch.rand(10, 1) * 2  # Increased weights for more input current\n",
        "\n",
        "        # STDP Learning parameters\n",
        "        self.stdp_eta = 0.005  # Learning rate for STDP\n",
        "\n",
        "    def forward(self, input_spikes):\n",
        "        # Input -> Hidden Layer\n",
        "        hidden_inputs = torch.matmul(input_spikes, self.synaptic_weights_ih)\n",
        "        hidden_spikes = self.hidden_neurons(hidden_inputs)\n",
        "\n",
        "        # Hidden -> Output Layer\n",
        "        output_inputs = torch.matmul(hidden_spikes, self.synaptic_weights_ho)\n",
        "        output_spikes = self.output_neuron(output_inputs)\n",
        "\n",
        "        return hidden_spikes, output_spikes\n",
        "\n",
        "    def apply_stdp(self, pre_spikes, post_spikes):\n",
        "        delta_w = self.stdp_eta * torch.matmul(pre_spikes.T, post_spikes)\n",
        "        return delta_w\n",
        "\n",
        "# Training the network with STDP\n",
        "def train_snn(snn, input_data, n_steps=100):\n",
        "    for t in range(n_steps):\n",
        "        # Forward pass\n",
        "        pre_spikes, output_spikes = snn(input_data)\n",
        "\n",
        "        # STDP rule applied between hidden and output neurons\n",
        "        dw_ho = snn.apply_stdp(pre_spikes, output_spikes)\n",
        "        snn.synaptic_weights_ho += dw_ho\n",
        "\n",
        "    return output_spikes\n",
        "\n",
        "# Spike Rate Calculation\n",
        "def calculate_spike_rate(spike_train):\n",
        "    return torch.sum(spike_train, dim=0) / spike_train.shape[0]\n",
        "\n",
        "# Generate two different Poisson signals\n",
        "input_data_A = generate_poisson_tensor((100, 2), rate=10)  # Signal A\n",
        "input_data_B = generate_poisson_tensor((100, 2), rate=20)  # Signal B\n",
        "\n",
        "# Encode both signals using BSA encoding\n",
        "encoded_input_A = BSA_encoding(input_data_A)\n",
        "encoded_input_B = BSA_encoding(input_data_B)\n",
        "\n",
        "# Initialize SNN and train separately for each signal\n",
        "snn_A = SNN()  # Separate SNN for Signal A\n",
        "output_spikes_A = train_snn(snn_A, encoded_input_A)\n",
        "\n",
        "snn_B = SNN()  # Separate SNN for Signal B\n",
        "output_spikes_B = train_snn(snn_B, encoded_input_B)\n",
        "\n",
        "# Plot output neuron spike trains for Signal A and Signal B\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot for Signal A\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(output_spikes_A.detach().numpy(), color='blue', marker='o')\n",
        "plt.title('Output Neuron Spike Train for Signal A')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Spikes')\n",
        "\n",
        "# Plot for Signal B\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(output_spikes_B.detach().numpy(), color='red', marker='o')\n",
        "plt.title('Output Neuron Spike Train for Signal B')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Spikes')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate spike rates for both signals\n",
        "spike_rate_A = calculate_spike_rate(output_spikes_A)\n",
        "spike_rate_B = calculate_spike_rate(output_spikes_B)\n",
        "\n",
        "print(f\"Spike Rate for Signal A: {spike_rate_A.item()}\")\n",
        "print(f\"Spike Rate for Signal B: {spike_rate_B.item()}\")\n"
      ],
      "metadata": {
        "id": "R0sdnSkLo2Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide to Simulating a Simple Spiking Neural Network using Brian2"
      ],
      "metadata": {
        "id": "Qiis3iJfwDhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install brian2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1l4uojKwXal",
        "outputId": "a3c50938-bdbd-4f0c-c031-fcd9c46638ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting brian2\n",
            "  Downloading Brian2-2.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from brian2) (1.26.4)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from brian2) (3.0.11)\n",
            "Requirement already satisfied: sympy>=1.2 in /usr/local/lib/python3.10/dist-packages (from brian2) (1.13.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from brian2) (3.1.4)\n",
            "Requirement already satisfied: jinja2>=2.7 in /usr/local/lib/python3.10/dist-packages (from brian2) (3.1.4)\n",
            "Requirement already satisfied: setuptools>=61 in /usr/local/lib/python3.10/dist-packages (from brian2) (71.0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from brian2) (24.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.7->brian2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.2->brian2) (1.3.0)\n",
            "Downloading Brian2-2.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brian2\n",
            "Successfully installed brian2-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from brian2 import *\n",
        "\n",
        "# Define model parameters\n",
        "tau = 20*ms  # Faster neuron dynamics\n",
        "synaptic_strength = 0.5  # Increased synaptic strength\n",
        "threshold_value = 0.8  # Threshold for spiking\n",
        "\n",
        "# Define model equations\n",
        "eqs = '''\n",
        "dv/dt = (-v) / tau : 1  # LIF model equation with decay towards zero\n",
        "'''\n",
        "\n",
        "# Create a neuron group\n",
        "N = 8  # number of neurons\n",
        "G = NeuronGroup(N, eqs, threshold=f'v>{threshold_value}', reset='v=0', method='exact')\n",
        "G.v = 'rand()'  # initialize membrane potential with random values\n",
        "\n",
        "# Connect neurons\n",
        "S = Synapses(G, G, on_pre=f'v_post += {synaptic_strength}')  # set synaptic strength\n",
        "S.connect(p=0.2)  # randomly connect neurons with higher connection probability\n",
        "\n",
        "# Monitor spikes\n",
        "spike_monitor = SpikeMonitor(G)\n",
        "\n",
        "# Monitor neuron membrane potentials for raster plot\n",
        "state_monitor = StateMonitor(G, 'v', record=True)\n",
        "\n",
        "# Create a network and add all components\n",
        "net = Network(G, S, spike_monitor, state_monitor)\n",
        "\n",
        "# Run the simulation\n",
        "net.run(2*second)\n",
        "\n",
        "# Visualize the spike raster plot\n",
        "figure(figsize=(12, 4))\n",
        "\n",
        "subplot(121)\n",
        "plot(spike_monitor.t/ms, spike_monitor.i, '.k')\n",
        "xlabel('Time (ms)')\n",
        "ylabel('Neuron index')\n",
        "title('Spike Raster Plot')\n",
        "\n",
        "# Plot membrane potentials for all neurons\n",
        "subplot(122)\n",
        "for i in range(N):\n",
        "    plot(state_monitor.t/ms, state_monitor.v[i])\n",
        "xlabel('Time (ms)')\n",
        "ylabel('Membrane potential (v)')\n",
        "title('Neuron Membrane Potentials')\n",
        "\n",
        "show()\n"
      ],
      "metadata": {
        "id": "AsL5zm4kwosl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}